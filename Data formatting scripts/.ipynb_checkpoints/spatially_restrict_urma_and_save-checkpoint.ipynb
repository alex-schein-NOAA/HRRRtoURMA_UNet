{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ee2312-c817-415a-82c5-b352bcc3fd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "from datetime import date, timedelta\n",
    "import xarray as xr\n",
    "import netCDF4 as nc\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a5011a-b00b-4dfb-bca9-94f272c729da",
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX_MIN_LON = 796 \n",
    "IDX_MIN_LAT = 645 \n",
    "\n",
    "IMG_SIZE_LON = 180\n",
    "IMG_SIZE_LAT = 180\n",
    "\n",
    "TIME_LIST = [str(i).zfill(2) for i in range(24)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031a5f22-3ca7-4757-8a6d-1ea62600fbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_URMA_ORIGINAL = \"/data1/ai-datadepot/models/urma/2p5km/grib2\"\n",
    "PATH_URMA_TRAIN = \"/scratch/RTMA/alex.schein/URMA_train_test/train\"\n",
    "PATH_URMA_TEST = \"/scratch/RTMA/alex.schein/URMA_train_test/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b53d74-6830-44fd-8325-57a4442081d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_DATE_TRAIN = date(2021,1,1) #should be jan 1, 2021\n",
    "END_DATE_TRAIN = date(2023,12,31) #should be dec 31, 2023\n",
    "NUM_DAYS_TRAIN = END_DATE_TRAIN-START_DATE_TRAIN\n",
    "\n",
    "START_DATE_TEST = date(2024,1,1) #should be jan 1, 2024\n",
    "END_DATE_TEST = date(2024,12,31) #should be dec 31, 2024\n",
    "NUM_DAYS_TEST = END_DATE_TEST-START_DATE_TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6853c8f5-4fb8-4754-8cb4-860997f9adb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### DIAGNOSE MISSING FILES\n",
    "\n",
    "## As of 6/23, no files are missing\n",
    "\n",
    "# for i in range(NUM_DAYS_TRAIN.days + 1):\n",
    "#     DATE_STR_TRAIN = date.strftime(START_DATE_TRAIN + timedelta(days=i), \"%Y%m%d\")\n",
    "#     filenames = os.listdir(PATH_URMA_ORIGINAL+\"/\"+DATE_STR_TRAIN)\n",
    "#     for time in TIME_LIST:\n",
    "#         try:\n",
    "#             filename = [x for x in filenames if time in x and \".idx\" not in x][0] #will only be one matching filename @ appropriate time\n",
    "#         except:\n",
    "#             print(f\"{DATE_STR_TRAIN} | {time}z is missing\")\n",
    "\n",
    "# for i in range(NUM_DAYS_TEST.days + 1):\n",
    "#     DATE_STR_TEST = date.strftime(START_DATE_TEST + timedelta(days=i), \"%Y%m%d\")\n",
    "#     filenames = os.listdir(PATH_URMA_ORIGINAL+\"/\"+DATE_STR_TEST)\n",
    "#     for time in TIME_LIST:\n",
    "#         try:\n",
    "#             filename = [x for x in filenames if time in x and \".idx\" not in x][0] #will only be one matching filename @ appropriate time\n",
    "#         except:\n",
    "#             print(f\"{DATE_STR_TEST} | {time}z is missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4b587c-d207-499c-9087-92ab9d247a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All 2021-2023 files to PATH_TRAIN directory\n",
    "for i in range(NUM_DAYS_TRAIN.days + 1):\n",
    "    DATE_STR_TRAIN = date.strftime(START_DATE_TRAIN + timedelta(days=i), \"%Y%m%d\")\n",
    "    filenames = os.listdir(PATH_URMA_ORIGINAL+\"/\"+DATE_STR_TRAIN)\n",
    "    # temp_filenames = os.listdir(PATH_URMA_TEMP)\n",
    "    for time in TIME_LIST:\n",
    "        try: #missing some 2021/04/19 files, as of 5/23\n",
    "            filename = [x for x in filenames if time in x and \".idx\" not in x][0] #will only be one matching filename @ appropriate time\n",
    "            new_filename = f\"urma_{DATE_STR_TRAIN}_t{time}z.nc\"\n",
    "            if not os.path.exists(PATH_URMA_TRAIN+f\"/{new_filename}\"):\n",
    "                #throws a hissy fit as it can't write an index file on ai-datadepot, but it should still compute fine...\n",
    "                t2m = xr.open_dataset(PATH_URMA_ORIGINAL+\"/\"+DATE_STR_TRAIN+\"/\"+filename, engine='cfgrib', decode_timedelta=True)\n",
    "                t2m = t2m.t2m\n",
    "                t2m_subset = t2m.isel(y=slice(IDX_MIN_LAT, IDX_MIN_LAT+IMG_SIZE_LAT),\n",
    "                                      x=slice(IDX_MIN_LON, IDX_MIN_LON+IMG_SIZE_LON))\n",
    "                t2m_subset.to_netcdf(PATH_URMA_TRAIN+\"/\"+new_filename)#, encoding={\"t2m\":{\"zlib\":True, \"complevel\":9}}) #(6/16) DON'T USE ENCODING - though it saves space, it automatically enables chunking which is SUPER slow!\n",
    "                print(f\"{new_filename} written to {PATH_URMA_TRAIN}\")\n",
    "        except: #This is a hack workaround for those missing 2021/04/19 files and should be removed once those are done\n",
    "            print(f\"{DATE_STR_TRAIN} | {time}z is missing\")\n",
    "            # try:\n",
    "            #     filename = [x for x in temp_filenames if time in x and \".idx\" not in x][0] #will only be one matching filename @ appropriate time\n",
    "            #     new_filename = f\"urma_{DATE_STR_TRAIN}_t{time}z.nc\"\n",
    "            #     if not os.path.exists(PATH_URMA_TRAIN+f\"/{new_filename}\"):\n",
    "            #         t2m = xr.open_dataset(PATH_URMA_TEMP+\"/\"+filename, engine='cfgrib', decode_timedelta=True)\n",
    "            #         t2m = t2m.t2m\n",
    "            #         t2m_subset = t2m.isel(y=slice(IDX_MIN_LAT, IDX_MIN_LAT+IMG_SIZE_LAT),\n",
    "            #                               x=slice(IDX_MIN_LON, IDX_MIN_LON+IMG_SIZE_LON))\n",
    "            #         t2m_subset.to_netcdf(PATH_URMA_TRAIN+\"/\"+new_filename, encoding={\"t2m\":{\"zlib\":True, \"complevel\":9}})\n",
    "            #         print(f\"{new_filename} written to {PATH_URMA_TRAIN}\")\n",
    "            # except:\n",
    "            #     print(f\"{DATE_STR_TRAIN} | {time}z is REALLY missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a9d657-81d1-4dac-9731-b7e394a2f535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All 2024 files to PATH_TEST directory\n",
    "# for i in range(NUM_DAYS_TEST.days + 1):\n",
    "for i in range(NUM_DAYS_TEST.days + 1):\n",
    "    DATE_STR_TEST = date.strftime(START_DATE_TEST + timedelta(days=i), \"%Y%m%d\")\n",
    "    filenames = os.listdir(PATH_URMA_ORIGINAL+\"/\"+DATE_STR_TEST)\n",
    "    for time in TIME_LIST:\n",
    "        filename = [x for x in filenames if time in x and \".idx\" not in x][0] #will only be one matching filename @ appropriate time\n",
    "        new_filename = f\"urma_{DATE_STR_TEST}_t{time}z.nc\"\n",
    "        if not os.path.exists(PATH_URMA_TEST+f\"/{new_filename}\"):\n",
    "            #throws a hissy fit as it can't write an index file on ai-datadepot, but it should still compute fine...\n",
    "            t2m = xr.open_dataset(PATH_URMA_ORIGINAL+\"/\"+DATE_STR_TEST+\"/\"+filename, engine='cfgrib', decode_timedelta=True)\n",
    "            t2m = t2m.t2m\n",
    "            t2m_subset = t2m.isel(y=slice(IDX_MIN_LAT, IDX_MIN_LAT+IMG_SIZE_LAT),\n",
    "                                  x=slice(IDX_MIN_LON, IDX_MIN_LON+IMG_SIZE_LON))\n",
    "            t2m_subset.to_netcdf(PATH_URMA_TEST+\"/\"+new_filename)#, encoding={\"t2m\":{\"zlib\":True, \"complevel\":9}})\n",
    "            print(f\"{new_filename} written to {PATH_URMA_TEST}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218ad508-e433-457b-9ee4-43eea8e8db46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
