{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40d80a21-cdd3-4a48-858e-58264e91f9c7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchinfo import summary\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import datetime as dt\n",
    "from netCDF4 import Dataset as nc_Dataset\n",
    "from netCDF4 import date2num, num2date\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import xarray as xr\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.markers import MarkerStyle\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "from HRRR_URMA_Datasets_AllVars import *\n",
    "from DefineModelAttributes import *\n",
    "from SR_UNet_simple import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddd4ee0e-3cd4-41c7-b0bb-15309f4833c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_LOG_FILEPATH = \"/scratch/RTMA/alex.schein/CNN_Main/Training_logs/training_log.txt\"\n",
    "\n",
    "NUM_GPUS_TO_USE = 3\n",
    "\n",
    "BATCH_SIZE = 256*NUM_GPUS_TO_USE #Be careful when initializing with this; should use BATCH_SIZE in DataLoader but use BATCH_SIZE/NUM_GPUS in model setup call\n",
    "NUM_EPOCHS = 1000\n",
    "\n",
    "PREDICTOR_VAR_LIST = [\"t2m\", \"d2m\", \"pressurf\", \"u10m\", \"v10m\"]\n",
    "TARGET_VAR_LIST = [[\"u10m\", \"v10m\"]]#[\"d2m\", \"pressurf\", \"u10m\", \"v10m\", [\"u10m\", \"v10m\"]] #don't need t2m, already have that. Also want a model that targets both u and v at the same time, but only those vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17246228-26c6-4ff8-9897-04a6a14ab1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terrain normalization done for ['diff']\n",
      "Loading predictor dataset for t2m (Years = 2021/22/23, months = 1 to 12, hours = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23])\n",
      "Predictor dataset data loaded. Time taken = 5.8 sec\n",
      "Normalizing over all times\n",
      "Normalization done. Time taken = 1.6 sec\n",
      "Loading predictor dataset for d2m (Years = 2021/22/23, months = 1 to 12, hours = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23])\n",
      "Predictor dataset data loaded. Time taken = 5.3 sec\n",
      "Normalizing over all times\n",
      "Normalization done. Time taken = 1.6 sec\n",
      "Loading predictor dataset for pressurf (Years = 2021/22/23, months = 1 to 12, hours = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23])\n",
      "Predictor dataset data loaded. Time taken = 5.1 sec\n",
      "Normalizing over all times\n",
      "Normalization done. Time taken = 1.6 sec\n",
      "Loading predictor dataset for u10m (Years = 2021/22/23, months = 1 to 12, hours = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23])\n",
      "Predictor dataset data loaded. Time taken = 4.9 sec\n",
      "Normalizing over all times\n",
      "Normalization done. Time taken = 1.6 sec\n",
      "Loading predictor dataset for v10m (Years = 2021/22/23, months = 1 to 12, hours = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23])\n",
      "Predictor dataset data loaded. Time taken = 5.7 sec\n",
      "Normalizing over all times\n",
      "Normalization done. Time taken = 1.6 sec\n",
      "Loading target dataset for u10m (Years = 2021/22/23, months = 1 to 12, hours = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23])\n",
      "Target dataset data loaded. Time taken = 7.6 sec\n",
      "Normalizing over all times\n",
      "Normalization done. Time taken = 1.6 sec\n",
      "Loading target dataset for v10m (Years = 2021/22/23, months = 1 to 12, hours = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23])\n",
      "Target dataset data loaded. Time taken = 5.3 sec\n",
      "Normalizing over all times\n",
      "Normalization done. Time taken = 1.6 sec\n",
      "DATASET CONSTRUCTION DONE\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(TARGET_VAR_LIST)):\n",
    "    current_model = DefineModelAttributes(predictor_vars=PREDICTOR_VAR_LIST,\n",
    "                                         target_vars=(TARGET_VAR_LIST[i] if isinstance(TARGET_VAR_LIST[i],list) else [TARGET_VAR_LIST[i]]))\n",
    "\n",
    "    current_model.create_dataset()\n",
    "    current_model_dataloader = DataLoader(current_model.dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4*NUM_GPUS_TO_USE, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b1c39ce-39ed-4b5d-a65b-335af6f178c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(current_model.dataset[0][1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4079872-26f9-4e47-a5b8-79d15b5845e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): SR_UNet_simple(\n",
       "    (first_conv): FirstConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(6, 64, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "        (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (second_conv): ConvBlock(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
       "        (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (enc_1): Encoder(\n",
       "      (encoder): Sequential(\n",
       "        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (1): ConvBlock(\n",
       "          (conv): Sequential(\n",
       "            (0): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
       "            (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (enc_2): Encoder(\n",
       "      (encoder): Sequential(\n",
       "        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (1): ConvBlock(\n",
       "          (conv): Sequential(\n",
       "            (0): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
       "            (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (enc_3): Encoder(\n",
       "      (encoder): Sequential(\n",
       "        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (1): ConvBlock(\n",
       "          (conv): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
       "            (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (enc_4): Encoder(\n",
       "      (encoder): Sequential(\n",
       "        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "        (1): ConvBlock(\n",
       "          (conv): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
       "            (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (dec_1): Decoder(\n",
       "      (conv): Sequential(\n",
       "        (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "        (1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "      )\n",
       "      (conv_block): ConvBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(1024, 512, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
       "          (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (dec_2): Decoder(\n",
       "      (conv): Sequential(\n",
       "        (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "      )\n",
       "      (conv_block): ConvBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(512, 256, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
       "          (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (dec_3): Decoder(\n",
       "      (conv): Sequential(\n",
       "        (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "        (1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "      )\n",
       "      (conv_block): ConvBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(256, 128, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
       "          (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (dec_4): Decoder(\n",
       "      (conv): Sequential(\n",
       "        (0): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "        (1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "      )\n",
       "      (conv_block): ConvBlock(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(128, 64, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
       "          (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_conv): FinalConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1), padding=same)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SR_UNet_simple(n_channels_in=current_model.num_channels_in, n_channels_out=current_model.num_channels_out)\n",
    "device = torch.device(\"cuda\")\n",
    "model = nn.DataParallel(model, device_ids=[i for i in range(NUM_GPUS_TO_USE)])\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, betas=[0.5,0.999]) #torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "loss_function = torch.nn.L1Loss()\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94ae70cf-f373-4889-9677-bb4b124e3cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels = next(iter(current_model_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b225530-c097-46a0-8d76-90bd89fe603e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([768, 2, 180, 180])) that is different to the input size (torch.Size([768, 1, 180, 180])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "inputs = inputs.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    "outputs = model(inputs.float()) #weird datatype mismatching... for some reason it's seeing HRRR data as double\n",
    "loss = loss_function(outputs,labels)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "551ea69d-89b7-4294-ae18-dc967709189f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 2, 180, 180])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf36ed2-2fa2-4479-84a7-ca6f1563d504",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
