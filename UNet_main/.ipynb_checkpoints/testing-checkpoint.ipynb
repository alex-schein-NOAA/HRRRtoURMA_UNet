{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40d80a21-cdd3-4a48-858e-58264e91f9c7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from FunctionsAndClasses.HEADER_torch import *\n",
    "from FunctionsAndClasses.HEADER_utilities import *\n",
    "from FunctionsAndClasses.HEADER_FunctionsAndClasses import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d263fa0f-0a1a-4827-ae2f-45998aae9073",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1d02bca-3bf5-4830-9384-7960cfe1523a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making dataset for model BS18_NE5_tD_pred(t2m)_targ(t2m)\n",
      "is_train = True\n",
      "Terrain normalization done for ['diff']\n",
      "Predictor data for t2m loaded. Time taken = 0.2 sec\n",
      "Target data for t2m loaded. Time taken = 0.2 sec\n",
      "DATASET CONSTRUCTION DONE\n",
      "Done with batch 7/730. Time taken = 1.6 sec. Running loss = 0.69299\n",
      "Done with batch 14/730. Time taken = 1.6 sec. Running loss = 0.47768\n",
      "Done with batch 21/730. Time taken = 1.6 sec. Running loss = 0.35952\n",
      "Done with batch 28/730. Time taken = 1.6 sec. Running loss = 0.29901\n",
      "Done with batch 35/730. Time taken = 1.6 sec. Running loss = 0.26271\n",
      "Done with batch 42/730. Time taken = 1.6 sec. Running loss = 0.23787\n",
      "Done with batch 49/730. Time taken = 1.6 sec. Running loss = 0.21885\n",
      "Done with batch 56/730. Time taken = 1.6 sec. Running loss = 0.20525\n",
      "Done with batch 63/730. Time taken = 1.6 sec. Running loss = 0.19390\n",
      "Done with batch 70/730. Time taken = 1.6 sec. Running loss = 0.18561\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      3\u001b[39m current_model = DefineModelAttributes(is_train=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      4\u001b[39m                                       with_terrains=[\u001b[33m'\u001b[39m\u001b[33mdiff\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m      5\u001b[39m                                       predictor_vars=[\u001b[33m'\u001b[39m\u001b[33mt2m\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m      6\u001b[39m                                       target_vars=[\u001b[33m'\u001b[39m\u001b[33mt2m\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m      7\u001b[39m                                       BATCH_SIZE=\u001b[32m18\u001b[39m,\n\u001b[32m      8\u001b[39m                                       NUM_EPOCHS=\u001b[32m5\u001b[39m)\n\u001b[32m     10\u001b[39m TRAINING_LOG_FILEPATH = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mC.DIR_UNET_MAIN\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/20250901_training_residual_test.txt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m TrainOneModel(current_model, \n\u001b[32m     13\u001b[39m               is_attention_model=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     14\u001b[39m               is_residual_model=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     15\u001b[39m               use_residual_block=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     16\u001b[39m               NUM_GPUS_TO_USE=\u001b[32m2\u001b[39m, \n\u001b[32m     17\u001b[39m               TRAINING_LOG_FILEPATH=TRAINING_LOG_FILEPATH, \n\u001b[32m     18\u001b[39m               TRAINED_MODEL_SAVEPATH=C.DIR_TRAINED_MODELS)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch3/BMC/wrfruc/aschein/UNet_main/FunctionsAndClasses/utils.py:135\u001b[39m, in \u001b[36mTrainOneModel\u001b[39m\u001b[34m(current_model, is_attention_model, is_residual_model, use_residual_block, NUM_GPUS_TO_USE, TRAINING_LOG_FILEPATH, TRAINED_MODEL_SAVEPATH)\u001b[39m\n\u001b[32m    133\u001b[39m outputs = model(inputs.float()) \u001b[38;5;66;03m#weird datatype mismatching... for some reason it's seeing HRRR data as double\u001b[39;00m\n\u001b[32m    134\u001b[39m loss = loss_function(outputs,labels)\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m loss.backward()\n\u001b[32m    136\u001b[39m optimizer.step()\n\u001b[32m    138\u001b[39m epoch_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch3/BMC/wrfruc/aschein/miniconda/envs/ML_environment/lib/python3.11/site-packages/torch/_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m torch.autograd.backward(\n\u001b[32m    648\u001b[39m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs=inputs\n\u001b[32m    649\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch3/BMC/wrfruc/aschein/miniconda/envs/ML_environment/lib/python3.11/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m _engine_run_backward(\n\u001b[32m    355\u001b[39m     tensors,\n\u001b[32m    356\u001b[39m     grad_tensors_,\n\u001b[32m    357\u001b[39m     retain_graph,\n\u001b[32m    358\u001b[39m     create_graph,\n\u001b[32m    359\u001b[39m     inputs_tuple,\n\u001b[32m    360\u001b[39m     allow_unreachable=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    361\u001b[39m     accumulate_grad=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    362\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch3/BMC/wrfruc/aschein/miniconda/envs/ML_environment/lib/python3.11/site-packages/torch/autograd/graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable._execution_engine.run_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    830\u001b[39m         t_outputs, *args, **kwargs\n\u001b[32m    831\u001b[39m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "C = CONSTANTS()\n",
    "\n",
    "current_model = DefineModelAttributes(is_train=True,\n",
    "                                      with_terrains=['diff'],\n",
    "                                      predictor_vars=['t2m'],\n",
    "                                      target_vars=['t2m'],\n",
    "                                      BATCH_SIZE=18,\n",
    "                                      NUM_EPOCHS=5)\n",
    "\n",
    "TRAINING_LOG_FILEPATH = f\"{C.DIR_UNET_MAIN}/20250901_training_residual_test.txt\"\n",
    "\n",
    "TrainOneModel(current_model, \n",
    "              is_attention_model=False,\n",
    "              is_residual_model=True,\n",
    "              use_residual_block=False,\n",
    "              NUM_GPUS_TO_USE=2, \n",
    "              TRAINING_LOG_FILEPATH=TRAINING_LOG_FILEPATH, \n",
    "              TRAINED_MODEL_SAVEPATH=C.DIR_TRAINED_MODELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebfcc3e-c0f4-4681-b2c8-d30a3468d1bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927142f6-88c2-418b-a67b-f66acfca24e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7a16b8-88ce-4835-8cc0-e92c9a73872e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
