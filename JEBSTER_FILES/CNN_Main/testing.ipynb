{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40d80a21-cdd3-4a48-858e-58264e91f9c7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchinfo import summary\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import datetime as dt\n",
    "from netCDF4 import Dataset as nc_Dataset\n",
    "from netCDF4 import date2num, num2date\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import xarray as xr\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.markers import MarkerStyle\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "import mpl_scatter_density\n",
    "from mpl_scatter_density import ScatterDensityArtist\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from FunctionsAndClasses.HRRR_URMA_Datasets_AllVars import *\n",
    "from FunctionsAndClasses.SR_UNet_simple import *\n",
    "from FunctionsAndClasses.DefineModelAttributes import *\n",
    "from FunctionsAndClasses.StatObjectConstructor import *\n",
    "from FunctionsAndClasses.utils import *\n",
    "from FunctionsAndClasses.CONSTANTS import *\n",
    "\n",
    "from FunctionsAndClasses.Dataset_TESTING import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14517459-e767-4a6e-94e0-0d2905ed0368",
   "metadata": {},
   "outputs": [],
   "source": [
    "HRRR_PATH = f\"/data1/projects/RTMA/alex.schein/Regridded_HRRR_train_test/\"\n",
    "URMA_PATH = f\"/data1/projects/RTMA/alex.schein/URMA_train_test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "058bfd66-55e9-4750-92ed-69d757745d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_name = 't2m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a768f198-b1b8-4c0c-9f57-1fc034df2bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model_attrs = DefineModelAttributes(is_train=False,\n",
    "                                         with_terrains=['diff'],\n",
    "                                         predictor_vars=['t2m'],\n",
    "                                         target_vars=['t2m'],\n",
    "                                         BATCH_SIZE=18,\n",
    "                                         NUM_EPOCHS=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1009b60-1fbd-4d74-a035-2de8761c645b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_GPUS_TO_USE=3\n",
    "MULTIGPU_BATCH_SIZE = test_model_attrs.BATCH_SIZE*NUM_GPUS_TO_USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "286bf1ea-43e3-485d-8183-3a5efaf6b1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring index file '/data1/projects/RTMA/alex.schein/Regridded_HRRR_train_test/train_hrrr_alltimes_t2m_f01.grib2.5b7b6.idx' incompatible with GRIB file\n",
      "Ignoring index file '/data1/projects/RTMA/alex.schein/URMA_train_test/train_urma_alltimes_t2m.grib2.5b7b6.idx' incompatible with GRIB file\n",
      "Ignoring index file '/data1/projects/RTMA/alex.schein/Terrain_Maps/terrain_subset_HRRR_2p5km.grib2.5b7b6.idx' incompatible with GRIB file\n",
      "Ignoring index file '/data1/projects/RTMA/alex.schein/Terrain_Maps/terrain_subset_URMA_2p5km.grib2.5b7b6.idx' incompatible with GRIB file\n"
     ]
    }
   ],
   "source": [
    "ds_test = Dataset_TEST(load_xr_into_memory=False, with_terrain_diff=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e72a013c-458f-4426-a64f-449590c57bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_test = DataLoader(ds_test, \n",
    "                     batch_size=MULTIGPU_BATCH_SIZE, \n",
    "                     shuffle=True, \n",
    "                     num_workers=4*NUM_GPUS_TO_USE, \n",
    "                     pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "418bf864-23a3-4dab-a827-f8896dbc39c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA driver initialization failed, you might not have a CUDA gpu.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# model = nn.DataParallel(model, device_ids=[i for i in range(NUM_GPUS_TO_USE)])\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m, betas\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.5\u001b[39m,\u001b[38;5;241m0.999\u001b[39m])\n\u001b[1;32m      9\u001b[0m loss_function \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mL1Loss()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1321\u001b[0m             device,\n\u001b[1;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1323\u001b[0m             non_blocking,\n\u001b[1;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1325\u001b[0m         )\n\u001b[0;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py:319\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    318\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 319\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    323\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA driver initialization failed, you might not have a CUDA gpu."
     ]
    }
   ],
   "source": [
    "model = SR_UNet_simple(n_channels_in=2, \n",
    "                        n_channels_out=1)\n",
    "device = torch.device(\"cuda\")\n",
    "# model = nn.DataParallel(model, device_ids=[i for i in range(NUM_GPUS_TO_USE)])\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, betas=[0.5,0.999])\n",
    "\n",
    "loss_function = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de601429-e709-4a5e-909d-f887946a8729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4e2100-34a7-4636-8aa5-c26309ddcc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea385ea2-3aef-41b7-b93b-1447ffaecad5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lowest_loss = 999\n",
    "# for epoch in range(1,test_model_attrs.NUM_EPOCHS+1):\n",
    "#     epoch_loss = 0.0\n",
    "#     start = time.time()\n",
    "#     for i, (inputs,labels) in enumerate(dl_test):    \n",
    "#         start_dl = time.time()\n",
    "#         print(f\"Starting batch {i+1}/{len(dl_test)} in epoch {epoch}\")\n",
    "#         inputs = inputs.to(device)\n",
    "#         labels = labels.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         outputs = model(inputs.float()) #weird datatype mismatching... for some reason it's seeing HRRR data as double\n",
    "#         loss = loss_function(outputs,labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         epoch_loss += loss.item()\n",
    "\n",
    "#         print(f\"Done with batch {i+1}/{len(dl_test)}. Time taken = {time.time()-start_dl:.1f} sec. Running loss = {epoch_loss/(i+1):.5f}\")\n",
    "        \n",
    "#     end = time.time() \n",
    "    \n",
    "#     if epoch_loss <= lowest_loss: #only save models that have lower loss than previous best\n",
    "#         lowest_loss = epoch_loss\n",
    "\n",
    "#     print(f\"End of epoch {epoch} | Average loss for epoch = {epoch_loss/len(dl_test):.5f} | Time for epoch = {end-start:.1f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acd8578-dff3-457e-b825-cd85e8cede11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINED_MODEL_SAVEPATH = \"/scratch/RTMA/alex.schein/CNN_Main/Trained_models\"\n",
    "# torch.save(model.module.state_dict(), f\"{TRAINED_MODEL_SAVEPATH}/{test_model_attrs.savename}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4522284-7cb1-4dee-84d3-767ee0bc153a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1679da-de18-46de-9d47-42042d9381cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940d4cc0-efa9-4199-aaf5-8a74f35eab29",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, targ = ds_test[IDX]\n",
    "pred = pred[np.newaxis,:] \n",
    "pred_gpu = torch.from_numpy(pred).cuda(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_output = model(pred_gpu.float())\n",
    "    model_output = model_output.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2ae09f-ab70-4595-ae99-afec6e2e9c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = CONSTANTS()\n",
    "\n",
    "hrrr_mean = C.hrrr_means_dict['train'][f\"{var_name}\"]\n",
    "hrrr_std = C.hrrr_stddevs_dict['train'][f\"{var_name}\"]\n",
    "\n",
    "urma_mean = C.urma_means_dict['train'][f\"{var_name}\"]\n",
    "urma_std = C.urma_stddevs_dict['train'][f\"{var_name}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459b34cf-82fb-4c54-9e6e-62fd6cfaa337",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(pred.squeeze()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9887744c-3058-4257-8c29-16a916a9e39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_unnormed = hrrr_std*pred.squeeze()[0] + hrrr_mean\n",
    "model_output_unnormed = urma_std*model_output + urma_mean\n",
    "targ_unnormed = urma_std*targ + urma_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7889d771-9ef3-45dd-8436-b85e6ba45499",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictor_output_truth_error(pred_unnormed, \n",
    "                                  model_output_unnormed, \n",
    "                                  targ_unnormed, \n",
    "                                  title=test_model_attrs.savename, \n",
    "                                  date_str=f\"IDX {IDX}\",\n",
    "                                  to_save=False,\n",
    "                                  save_dir=f\"/scratch/RTMA/alex.schein/\", \n",
    "                                  fig_savename=f\"IDX{IDX}_{test_model_attrs.savename}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c906330-4c90-4aaa-aabf-88203d5b1085",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictor_output_truth_error(pred_unnormed, pred_unnormed, targ_unnormed, title=\"No model\", date_str=f\"IDX {IDX}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ea9038-793b-468a-a23a-ddf903c6af66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
